{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string, os\n",
    "import contractions\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, Callback\n",
    "from tensorflow.keras.layers import Concatenate, Dropout\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, CSVLogger\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_sentence</th>\n",
       "      <th>hindi_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Working out of his studio at Amir Mahal in Che...</td>\n",
       "      <td>चेन्नै के रोयपेट्टां में अपने स्टुड़ियो में का...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Temperature can be observed by inserting an or...</td>\n",
       "      <td>जानवर की गुदा में सामान्य थर्मामीटर डालकर उसका...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>but for a life jacket.</td>\n",
       "      <td>मगर बस एक लाइफ़-जैकेट ही बची थी।</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Background and next steps</td>\n",
       "      <td>पृष्ठभूमि तथा अगले कदम</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Main point: Detailed story of Mahabharat</td>\n",
       "      <td>मुख्य उल्लेख :महाभारत की विस्तृत कथा</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    english_sentence  \\\n",
       "0  Working out of his studio at Amir Mahal in Che...   \n",
       "1  Temperature can be observed by inserting an or...   \n",
       "2                             but for a life jacket.   \n",
       "3                          Background and next steps   \n",
       "4           Main point: Detailed story of Mahabharat   \n",
       "\n",
       "                                      hindi_sentence  \n",
       "0  चेन्नै के रोयपेट्टां में अपने स्टुड़ियो में का...  \n",
       "1  जानवर की गुदा में सामान्य थर्मामीटर डालकर उसका...  \n",
       "2                   मगर बस एक लाइफ़-जैकेट ही बची थी।  \n",
       "3                             पृष्ठभूमि तथा अगले कदम  \n",
       "4               मुख्य उल्लेख :महाभारत की विस्तृत कथा  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_df = pd.read_csv('eng-hindi.csv').sample(frac=.06, random_state=3, replace=False).reset_index(drop=True)\n",
    "corpus_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processing(sentence, lang):\n",
    "    # strip any unneccesary spaces\n",
    "    sentence = sentence.strip()\n",
    "    # expand word contractions for english\n",
    "    if lang == 'eng':\n",
    "        sentence = contractions.fix(sentence)\n",
    "    sentence = sentence.lower()\n",
    "    # remove all punctuations from sentence '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "    sentence = sentence.translate(str.maketrans(\"\", \"\", string.punctuation + \"‘“”\"))\n",
    "    sentence = \" \".join([w for w in sentence.split()]) # de-constructing the sentence\n",
    "    # append <BOS> and <EOS> token\n",
    "    # EOS (end of a sentence) \n",
    "    # BOS (beginning of a sentence)\n",
    "    sentence = \"<BOS> \" + sentence + \" <EOS>\"\n",
    "    return sentence\n",
    "\n",
    "corpus_df.english_sentence = corpus_df.english_sentence.apply(data_processing, lang='eng')\n",
    "corpus_df.hindi_sentence = corpus_df.hindi_sentence.apply(data_processing, lang='hindi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english_sentence</th>\n",
       "      <th>hindi_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;BOS&gt; working out of his studio at amir mahal ...</td>\n",
       "      <td>&lt;BOS&gt; चेन्नै के रोयपेट्टां में अपने स्टुड़ियो ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;BOS&gt; temperature can be observed by inserting...</td>\n",
       "      <td>&lt;BOS&gt; जानवर की गुदा में सामान्य थर्मामीटर डालक...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;BOS&gt; but for a life jacket &lt;EOS&gt;</td>\n",
       "      <td>&lt;BOS&gt; मगर बस एक लाइफ़जैकेट ही बची थी। &lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;BOS&gt; background and next steps &lt;EOS&gt;</td>\n",
       "      <td>&lt;BOS&gt; पृष्ठभूमि तथा अगले कदम &lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;BOS&gt; main point detailed story of mahabharat ...</td>\n",
       "      <td>&lt;BOS&gt; मुख्य उल्लेख महाभारत की विस्तृत कथा &lt;EOS&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    english_sentence  \\\n",
       "0  <BOS> working out of his studio at amir mahal ...   \n",
       "1  <BOS> temperature can be observed by inserting...   \n",
       "2                  <BOS> but for a life jacket <EOS>   \n",
       "3              <BOS> background and next steps <EOS>   \n",
       "4  <BOS> main point detailed story of mahabharat ...   \n",
       "\n",
       "                                      hindi_sentence  \n",
       "0  <BOS> चेन्नै के रोयपेट्टां में अपने स्टुड़ियो ...  \n",
       "1  <BOS> जानवर की गुदा में सामान्य थर्मामीटर डालक...  \n",
       "2        <BOS> मगर बस एक लाइफ़जैकेट ही बची थी। <EOS>  \n",
       "3                 <BOS> पृष्ठभूमि तथा अगले कदम <EOS>  \n",
       "4    <BOS> मुख्य उल्लेख महाभारत की विस्तृत कथा <EOS>  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_index, hindi_index = set(), set()\n",
    "\n",
    "for idx, (sen, sen1) in enumerate(zip(\n",
    "              corpus_df['english_sentence'],\n",
    "              corpus_df['hindi_sentence'])):\n",
    "    sen = sen.split()\n",
    "    sen1 = sen1.split()\n",
    "    if len(sen) > 20:\n",
    "        eng_index.add(idx)\n",
    "    if len(sen1) > 20:\n",
    "        hindi_index.add(idx)\n",
    "        \n",
    "indices = list(eng_index.union(hindi_index))\n",
    "corpus_df.drop(indices, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting into train and test set\n",
    "x, y = corpus_df['english_sentence'].to_numpy(), corpus_df['hindi_sentence'].to_numpy()\n",
    "\n",
    "eng_train, eng_test, hindi_train, hindi_test = train_test_split(x, y, test_size= .05, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve english and hindi vocabulary.\n",
    "eng_vocab = set()\n",
    "hindi_vocab = set()\n",
    "\n",
    "MAX_LEN_ENG = 0\n",
    "MAX_LEN_HIN = 0\n",
    "\n",
    "for idx, sen in enumerate(eng_train):\n",
    "    eng_words = sen.split()\n",
    "    # it returns max sentence length\n",
    "    MAX_LEN_ENG = max(MAX_LEN_ENG, len(eng_words))\n",
    "    for e_w in eng_words:\n",
    "        if e_w not in eng_vocab:\n",
    "            eng_vocab.add(e_w)\n",
    "\n",
    "for idx, sen in enumerate(hindi_train):\n",
    "    hindi_words = sen.split()\n",
    "    MAX_LEN_HIN = max(MAX_LEN_HIN, len(hindi_words))\n",
    "    for h_w in hindi_words:\n",
    "        if h_w not in hindi_vocab:\n",
    "            hindi_vocab.add(h_w)\n",
    "            \n",
    "# Adding unknown word tokens\n",
    "eng_vocab.add('UNK')\n",
    "hindi_vocab.add('UNK')\n",
    "MAX_LEN_ENG += 1\n",
    "MAX_LEN_HIN += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vocab_idx = {j : i for i, j in enumerate(eng_vocab, 1)}\n",
    "idx_vocab_en = dict(map(reversed, en_vocab_idx.items()))\n",
    "\n",
    "hin_vocab_idx = {j : i for i, j in enumerate(hindi_vocab, 1)}\n",
    "idx_vocab_hin = dict(map(reversed, hin_vocab_idx.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sentence length(i.e n words) ENGLISH: 21\n",
      "Maximum sentence length(i.e n words) HINDI: 21\n",
      "Size of the vocabulary (English) : 8071\n",
      "Size of the vocabulary (Hindi) : 9315\n"
     ]
    }
   ],
   "source": [
    "INPUT_VOCAB = len(eng_vocab) + 1\n",
    "TARGET_VOCAB = len(hindi_vocab) + 1\n",
    "\n",
    "print(f'Maximum sentence length(i.e n words) ENGLISH: {MAX_LEN_ENG}')\n",
    "print(f'Maximum sentence length(i.e n words) HINDI: {MAX_LEN_HIN}')\n",
    "print(f'Size of the vocabulary (English) : {INPUT_VOCAB}')\n",
    "print(f'Size of the vocabulary (Hindi) : {TARGET_VOCAB}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    \"\"\"\n",
    "    Generates batches \n",
    "    \"\"\"    \n",
    "    def __init__(self, eng_train, hindi_train, \n",
    "                 MAX_LEN_ENG, \n",
    "                 MAX_LEN_HIN,\n",
    "                 TARGET_VOCAB,\n",
    "                 batch_size, \n",
    "                 shuffle,\n",
    "                 training=True):        \n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.eng_train = eng_train\n",
    "        self.hindi_train = hindi_train\n",
    "        self.MAX_LEN_ENG = MAX_LEN_ENG\n",
    "        self.MAX_LEN_HIN = MAX_LEN_HIN\n",
    "        self.TARGET_VOCAB = TARGET_VOCAB\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.eng_train) // self.batch_size\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        eng_batch = [self.eng_train[i] for i in indexes]\n",
    "        hindi_batch = [self.hindi_train[i] for i in indexes]\n",
    "        input_data, decoder_target = self.__data_generation(eng_batch, hindi_batch)\n",
    "        return input_data, decoder_target\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.eng_train))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "            \n",
    "    def __data_generation(self, eng_batch, hindi_batch):\n",
    "        encoder_input = np.zeros((self.batch_size, self.MAX_LEN_ENG), dtype='float32')  \n",
    "        decoder_input = np.zeros((self.batch_size, self.MAX_LEN_HIN), dtype='float32')\n",
    "        decoder_target = np.zeros((self.batch_size, self.MAX_LEN_HIN, TARGET_VOCAB), dtype='float32')\n",
    "        \n",
    "        for enum, sen in enumerate(eng_batch):\n",
    "            sen = sen.split()\n",
    "            for x, e_word in enumerate(sen):\n",
    "                try:\n",
    "                    encoder_input[enum, x] = en_vocab_idx[e_word]\n",
    "                except KeyError:\n",
    "                    encoder_input[enum, x] = en_vocab_idx['UNK']\n",
    "        \n",
    "        for enum, sen in enumerate(hindi_batch):\n",
    "            sen = sen.split()\n",
    "            for x, h_word in enumerate(sen):\n",
    "                try:\n",
    "                    decoder_input[enum, x] = hin_vocab_idx[h_word]\n",
    "                    if x > 0:\n",
    "                        decoder_target[enum, x - 1, hin_vocab_idx[h_word]] = 1.\n",
    "                except KeyError:\n",
    "                    decoder_input[enum, x] = hin_vocab_idx['UNK']\n",
    "                    if x > 0:\n",
    "                        decoder_target[enum, x - 1, hin_vocab_idx['UNK']] = 1.\n",
    "        return [encoder_input, decoder_input], decoder_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 150\n",
    "\n",
    "## Encoder- SETUP\n",
    "encoder_inputs = Input(shape=(MAX_LEN_ENG,))\n",
    "\n",
    "encoder_emb_layer = Embedding(INPUT_VOCAB, latent_dim, mask_zero=True)(encoder_inputs)\n",
    "encoder_dropout1 = Dropout(0.4)(encoder_emb_layer)\n",
    "encoder_lstm_layer1 = Bidirectional(LSTM(latent_dim, return_sequences=True))(encoder_dropout1)\n",
    "encoder_lstm_layer2 = Bidirectional(LSTM(latent_dim, return_state=True))\n",
    "\n",
    "encoder_outputs, fstate_h, fstate_c, bstate_h, bstate_c = encoder_lstm_layer2(encoder_lstm_layer1)\n",
    "state_h = Concatenate()([fstate_h, bstate_h])\n",
    "state_c = Concatenate()([fstate_c, bstate_c])\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "## Decoder-SETUP\n",
    "decoder_inputs = Input(shape=(MAX_LEN_HIN,))\n",
    "decoder_emb_layer = Embedding(TARGET_VOCAB, latent_dim, mask_zero=True)\n",
    "decoder_dropout1 = Dropout(0.4)\n",
    "decoder_lstm_layer1 = LSTM(latent_dim*2, return_sequences=True)\n",
    "decoder_lstm_layer2 = LSTM(latent_dim*2, return_sequences=True, return_state=True)\n",
    "\n",
    "decoder_dropout1 = decoder_dropout1(decoder_emb_layer(decoder_inputs))\n",
    "decoder_outputs, _, _ = decoder_lstm_layer2(decoder_lstm_layer1(decoder_dropout1, initial_state=encoder_states))\n",
    "decoder_dense1 = Dense(latent_dim, activation=LeakyReLU(), kernel_initializer='he_uniform')\n",
    "decoder_dense2 = Dense(TARGET_VOCAB, activation='softmax')\n",
    "decoder_outputs = decoder_dense2(decoder_dense1(decoder_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"NMT\"\n",
      "_________________________________________________________________________________________________________\n",
      "Layer (type)                      Output Shape           Param #      Connected to                       \n",
      "=========================================================================================================\n",
      "input_3 (InputLayer)              [(None, 21)]           0                                               \n",
      "_________________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)           (None, 21, 150)        1210650      input_3[0][0]                      \n",
      "_________________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)               (None, 21, 150)        0            embedding_2[0][0]                  \n",
      "_________________________________________________________________________________________________________\n",
      "input_4 (InputLayer)              [(None, 21)]           0                                               \n",
      "_________________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional)   (None, 21, 300)        361200       dropout_2[0][0]                    \n",
      "_________________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)           (None, 21, 150)        1397250      input_4[0][0]                      \n",
      "_________________________________________________________________________________________________________\n",
      "bidirectional_3 (Bidirectional)   [(None, 300), (None, 1 541200       bidirectional_2[0][0]              \n",
      "_________________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)               (None, 21, 150)        0            embedding_3[0][0]                  \n",
      "_________________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)       (None, 300)            0            bidirectional_3[0][1]              \n",
      "                                                                      bidirectional_3[0][3]              \n",
      "_________________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)       (None, 300)            0            bidirectional_3[0][2]              \n",
      "                                                                      bidirectional_3[0][4]              \n",
      "_________________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                     (None, 21, 300)        541200       dropout_3[0][0]                    \n",
      "                                                                      concatenate_2[0][0]                \n",
      "                                                                      concatenate_3[0][0]                \n",
      "_________________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                     [(None, 21, 300), (Non 721200       lstm_4[0][0]                       \n",
      "_________________________________________________________________________________________________________\n",
      "dense (Dense)                     (None, 21, 150)        45150        lstm_5[0][0]                       \n",
      "_________________________________________________________________________________________________________\n",
      "dense_1 (Dense)                   (None, 21, 9315)       1406565      dense[0][0]                        \n",
      "=========================================================================================================\n",
      "Total params: 6,224,415\n",
      "Trainable params: 6,224,415\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nmt_model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=decoder_outputs, name='NMT')\n",
    "nmt_model.compile(optimizer=RMSprop(0.01), loss='categorical_crossentropy')\n",
    "nmt_model.summary(line_length=105)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Callback\n",
    "class _learning_rate(Callback) :\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        lr = float(backend.get_value(self.model.optimizer.lr))\n",
    "        print(f\"Learning Rate for epoch: {epoch}: \", lr)        \n",
    "\n",
    "# callbacks\n",
    "R_LR = ReduceLROnPlateau(monitor='val_loss', verbose=1, min_delta=1e-04, cooldown=2)\n",
    "LOG = CSVLogger('training1.log', separator=';', append=True)\n",
    "MC = ModelCheckpoint('saved/model-best-{epoch:03d}.h5', mode='min')\n",
    "\n",
    "# Remove any earlier saved models if any.\n",
    "if os.listdir('saved'):\n",
    "    os.system('rm -rf ./saved/*')\n",
    "        \n",
    "# Default configs\n",
    "batch_size = 128\n",
    "nb_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate for epoch: 0:  0.009999999776482582\n",
      "Epoch 1/100\n",
      "63/63 [==============================] - 37s 588ms/step - loss: 3.7889 - val_loss: 3.6126\n",
      "Learning Rate for epoch: 1:  0.009999999776482582\n",
      "Epoch 2/100\n",
      "63/63 [==============================] - 36s 567ms/step - loss: 3.2330 - val_loss: 3.4392\n",
      "Learning Rate for epoch: 2:  0.009999999776482582\n",
      "Epoch 3/100\n",
      "63/63 [==============================] - 36s 572ms/step - loss: 2.9866 - val_loss: 3.3521\n",
      "Learning Rate for epoch: 3:  0.009999999776482582\n",
      "Epoch 4/100\n",
      "63/63 [==============================] - 36s 569ms/step - loss: 2.7666 - val_loss: 3.3248\n",
      "Learning Rate for epoch: 4:  0.009999999776482582\n",
      "Epoch 5/100\n",
      "63/63 [==============================] - 36s 573ms/step - loss: 2.5526 - val_loss: 3.3332\n",
      "Learning Rate for epoch: 5:  0.009999999776482582\n",
      "Epoch 6/100\n",
      "63/63 [==============================] - 36s 569ms/step - loss: 2.3811 - val_loss: 3.4651\n",
      "Learning Rate for epoch: 6:  0.009999999776482582\n",
      "Epoch 7/100\n",
      "63/63 [==============================] - 36s 573ms/step - loss: 2.1813 - val_loss: 3.4482\n",
      "Learning Rate for epoch: 7:  0.009999999776482582\n",
      "Epoch 8/100\n",
      "63/63 [==============================] - 36s 570ms/step - loss: 1.9554 - val_loss: 3.6216\n",
      "Learning Rate for epoch: 8:  0.009999999776482582\n",
      "Epoch 9/100\n",
      "63/63 [==============================] - 36s 568ms/step - loss: 1.7839 - val_loss: 3.7102\n",
      "Learning Rate for epoch: 9:  0.009999999776482582\n",
      "Epoch 10/100\n",
      "63/63 [==============================] - 36s 571ms/step - loss: 1.6114 - val_loss: 3.7481\n",
      "Learning Rate for epoch: 10:  0.009999999776482582\n",
      "Epoch 11/100\n",
      "63/63 [==============================] - 36s 569ms/step - loss: 1.3933 - val_loss: 3.8754\n",
      "Learning Rate for epoch: 11:  0.009999999776482582\n",
      "Epoch 12/100\n",
      "63/63 [==============================] - 36s 569ms/step - loss: 1.2310 - val_loss: 4.0143\n",
      "Learning Rate for epoch: 12:  0.009999999776482582\n",
      "Epoch 13/100\n",
      "63/63 [==============================] - 36s 572ms/step - loss: 1.1175 - val_loss: 4.1183\n",
      "Learning Rate for epoch: 13:  0.009999999776482582\n",
      "Epoch 14/100\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.9874\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "63/63 [==============================] - 36s 570ms/step - loss: 0.9876 - val_loss: 4.1425\n",
      "Learning Rate for epoch: 14:  0.0009999999310821295\n",
      "Epoch 15/100\n",
      "63/63 [==============================] - 36s 569ms/step - loss: 0.6773 - val_loss: 4.1310\n",
      "Learning Rate for epoch: 15:  0.0009999999310821295\n",
      "Epoch 16/100\n",
      "63/63 [==============================] - 36s 575ms/step - loss: 0.5704 - val_loss: 4.1506\n",
      "Learning Rate for epoch: 16:  0.0009999999310821295\n",
      "Epoch 17/100\n",
      "63/63 [==============================] - 36s 569ms/step - loss: 0.5334 - val_loss: 4.1975\n",
      "Learning Rate for epoch: 17:  0.0009999999310821295\n",
      "Epoch 18/100\n",
      "63/63 [==============================] - 36s 570ms/step - loss: 0.4951 - val_loss: 4.2108\n",
      "Learning Rate for epoch: 18:  0.0009999999310821295\n",
      "Epoch 19/100\n",
      "63/63 [==============================] - 36s 570ms/step - loss: 0.4704 - val_loss: 4.3189\n",
      "Learning Rate for epoch: 19:  0.0009999999310821295\n",
      "Epoch 20/100\n",
      "63/63 [==============================] - 36s 569ms/step - loss: 0.4533 - val_loss: 4.3193\n",
      "Learning Rate for epoch: 20:  0.0009999999310821295\n",
      "Epoch 21/100\n",
      "63/63 [==============================] - 36s 569ms/step - loss: 0.4271 - val_loss: 4.3470\n",
      "Learning Rate for epoch: 21:  0.0009999999310821295\n",
      "Epoch 22/100\n",
      "63/63 [==============================] - 36s 571ms/step - loss: 0.4091 - val_loss: 4.3852\n",
      "Learning Rate for epoch: 22:  0.0009999999310821295\n",
      "Epoch 23/100\n",
      "63/63 [==============================] - 36s 570ms/step - loss: 0.3977 - val_loss: 4.4183\n",
      "Learning Rate for epoch: 23:  0.0009999999310821295\n",
      "Epoch 24/100\n",
      "63/63 [==============================] - 36s 567ms/step - loss: 0.3787 - val_loss: 4.4242\n",
      "Learning Rate for epoch: 24:  0.0009999999310821295\n",
      "Epoch 25/100\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.3598\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "63/63 [==============================] - 36s 576ms/step - loss: 0.3595 - val_loss: 4.4611\n",
      "Learning Rate for epoch: 25:  9.99999901978299e-05\n",
      "Epoch 26/100\n",
      "63/63 [==============================] - 36s 565ms/step - loss: 0.3390 - val_loss: 4.4899\n",
      "Learning Rate for epoch: 26:  9.99999901978299e-05\n",
      "Epoch 27/100\n",
      "63/63 [==============================] - 36s 566ms/step - loss: 0.3317 - val_loss: 4.4502\n",
      "Learning Rate for epoch: 27:  9.99999901978299e-05\n",
      "Epoch 28/100\n",
      "63/63 [==============================] - 36s 568ms/step - loss: 0.3269 - val_loss: 4.4430\n",
      "Learning Rate for epoch: 28:  9.99999901978299e-05\n",
      "Epoch 29/100\n",
      "63/63 [==============================] - 36s 567ms/step - loss: 0.3257 - val_loss: 4.4622\n",
      "Learning Rate for epoch: 29:  9.99999901978299e-05\n",
      "Epoch 30/100\n",
      "63/63 [==============================] - 36s 571ms/step - loss: 0.3229 - val_loss: 4.4564\n",
      "Learning Rate for epoch: 30:  9.99999901978299e-05\n",
      "Epoch 31/100\n",
      "63/63 [==============================] - 36s 566ms/step - loss: 0.3227 - val_loss: 4.4606\n",
      "Learning Rate for epoch: 31:  9.99999901978299e-05\n",
      "Epoch 32/100\n",
      "63/63 [==============================] - 36s 569ms/step - loss: 0.3172 - val_loss: 4.4868\n",
      "Learning Rate for epoch: 32:  9.99999901978299e-05\n",
      "Epoch 33/100\n",
      "63/63 [==============================] - 36s 569ms/step - loss: 0.3182 - val_loss: 4.4721\n",
      "Learning Rate for epoch: 33:  9.99999901978299e-05\n",
      "Epoch 34/100\n",
      "63/63 [==============================] - 36s 569ms/step - loss: 0.3181 - val_loss: 4.4853\n",
      "Learning Rate for epoch: 34:  9.99999901978299e-05\n",
      "Epoch 35/100\n",
      "63/63 [==============================] - 36s 566ms/step - loss: 0.3136 - val_loss: 4.4861\n",
      "Learning Rate for epoch: 35:  9.99999901978299e-05\n",
      "Epoch 36/100\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.3145\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "63/63 [==============================] - 36s 565ms/step - loss: 0.3144 - val_loss: 4.4949\n",
      "Learning Rate for epoch: 36:  9.99999883788405e-06\n",
      "Epoch 37/100\n",
      "63/63 [==============================] - 36s 567ms/step - loss: 0.3106 - val_loss: 4.4950\n",
      "Learning Rate for epoch: 37:  9.99999883788405e-06\n",
      "Epoch 38/100\n",
      "63/63 [==============================] - 36s 565ms/step - loss: 0.3089 - val_loss: 4.5280\n",
      "Learning Rate for epoch: 38:  9.99999883788405e-06\n",
      "Epoch 39/100\n",
      "63/63 [==============================] - 36s 567ms/step - loss: 0.3102 - val_loss: 4.4904\n",
      "Learning Rate for epoch: 39:  9.99999883788405e-06\n",
      "Epoch 40/100\n",
      "63/63 [==============================] - 36s 565ms/step - loss: 0.3094 - val_loss: 4.4813\n",
      "Learning Rate for epoch: 40:  9.99999883788405e-06\n",
      "Epoch 41/100\n",
      "63/63 [==============================] - 36s 567ms/step - loss: 0.3085 - val_loss: 4.5190\n",
      "Learning Rate for epoch: 41:  9.99999883788405e-06\n",
      "Epoch 42/100\n",
      "63/63 [==============================] - 36s 569ms/step - loss: 0.3101 - val_loss: 4.4954\n",
      "Learning Rate for epoch: 42:  9.99999883788405e-06\n",
      "Epoch 43/100\n",
      "63/63 [==============================] - 36s 567ms/step - loss: 0.3080 - val_loss: 4.5255\n",
      "Learning Rate for epoch: 43:  9.99999883788405e-06\n",
      "Epoch 44/100\n",
      "63/63 [==============================] - 36s 564ms/step - loss: 0.3102 - val_loss: 4.4962\n",
      "Learning Rate for epoch: 44:  9.99999883788405e-06\n",
      "Epoch 45/100\n",
      "63/63 [==============================] - 36s 567ms/step - loss: 0.3068 - val_loss: 4.4999\n",
      "Learning Rate for epoch: 45:  9.99999883788405e-06\n",
      "Epoch 46/100\n",
      "63/63 [==============================] - 36s 566ms/step - loss: 0.3093 - val_loss: 4.5105\n",
      "Learning Rate for epoch: 46:  9.99999883788405e-06\n",
      "Epoch 47/100\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.3077\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "63/63 [==============================] - 36s 574ms/step - loss: 0.3076 - val_loss: 4.4793\n",
      "Learning Rate for epoch: 47:  9.99999883788405e-07\n",
      "Epoch 48/100\n",
      "63/63 [==============================] - 36s 566ms/step - loss: 0.3058 - val_loss: 4.4931\n",
      "Learning Rate for epoch: 48:  9.99999883788405e-07\n",
      "Epoch 49/100\n",
      "63/63 [==============================] - 36s 565ms/step - loss: 0.3095 - val_loss: 4.4984\n",
      "Learning Rate for epoch: 49:  9.99999883788405e-07\n",
      "Epoch 50/100\n",
      "63/63 [==============================] - 36s 565ms/step - loss: 0.3064 - val_loss: 4.4764\n",
      "Learning Rate for epoch: 50:  9.99999883788405e-07\n",
      "Epoch 51/100\n",
      "63/63 [==============================] - 36s 569ms/step - loss: 0.3088 - val_loss: 4.5085\n",
      "Learning Rate for epoch: 51:  9.99999883788405e-07\n",
      "Epoch 52/100\n",
      "63/63 [==============================] - 36s 567ms/step - loss: 0.3069 - val_loss: 4.5276\n",
      "Learning Rate for epoch: 52:  9.99999883788405e-07\n",
      "Epoch 53/100\n",
      "63/63 [==============================] - 36s 564ms/step - loss: 0.3063 - val_loss: 4.5172\n",
      "Learning Rate for epoch: 53:  9.99999883788405e-07\n",
      "Epoch 54/100\n",
      "63/63 [==============================] - 36s 565ms/step - loss: 0.3084 - val_loss: 4.5018\n",
      "Learning Rate for epoch: 54:  9.99999883788405e-07\n",
      "Epoch 55/100\n",
      "63/63 [==============================] - 36s 566ms/step - loss: 0.3069 - val_loss: 4.5169\n",
      "Learning Rate for epoch: 55:  9.99999883788405e-07\n",
      "Epoch 56/100\n",
      "63/63 [==============================] - 36s 565ms/step - loss: 0.3062 - val_loss: 4.4972\n",
      "Learning Rate for epoch: 56:  9.99999883788405e-07\n",
      "Epoch 57/100\n",
      "63/63 [==============================] - 36s 565ms/step - loss: 0.3086 - val_loss: 4.4863\n",
      "Learning Rate for epoch: 57:  9.99999883788405e-07\n",
      "Epoch 58/100\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.3071\n",
      "Epoch 00058: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "63/63 [==============================] - 36s 565ms/step - loss: 0.3071 - val_loss: 4.4907\n",
      "Learning Rate for epoch: 58:  9.999998695775503e-08\n",
      "Epoch 59/100\n",
      "63/63 [==============================] - 36s 570ms/step - loss: 0.3069 - val_loss: 4.5064\n",
      "Learning Rate for epoch: 59:  9.999998695775503e-08\n",
      "Epoch 60/100\n",
      "63/63 [==============================] - 36s 564ms/step - loss: 0.3068 - val_loss: 4.5213\n",
      "Learning Rate for epoch: 60:  9.999998695775503e-08\n",
      "Epoch 61/100\n",
      "63/63 [==============================] - 36s 566ms/step - loss: 0.3059 - val_loss: 4.5114\n",
      "Learning Rate for epoch: 61:  9.999998695775503e-08\n",
      "Epoch 62/100\n",
      "63/63 [==============================] - 36s 566ms/step - loss: 0.3081 - val_loss: 4.4863\n",
      "Learning Rate for epoch: 62:  9.999998695775503e-08\n",
      "Epoch 63/100\n",
      "63/63 [==============================] - 36s 565ms/step - loss: 0.3079 - val_loss: 4.4914\n",
      "Learning Rate for epoch: 63:  9.999998695775503e-08\n",
      "Epoch 64/100\n",
      "63/63 [==============================] - 36s 565ms/step - loss: 0.3074 - val_loss: 4.4725\n",
      "Learning Rate for epoch: 64:  9.999998695775503e-08\n",
      "Epoch 65/100\n",
      "63/63 [==============================] - 36s 566ms/step - loss: 0.3072 - val_loss: 4.5195\n",
      "Learning Rate for epoch: 65:  9.999998695775503e-08\n",
      "Epoch 66/100\n",
      "63/63 [==============================] - 36s 565ms/step - loss: 0.3079 - val_loss: 4.4967\n",
      "Learning Rate for epoch: 66:  9.999998695775503e-08\n",
      "Epoch 67/100\n",
      "63/63 [==============================] - 36s 565ms/step - loss: 0.3066 - val_loss: 4.5044\n",
      "Learning Rate for epoch: 67:  9.999998695775503e-08\n",
      "Epoch 68/100\n",
      "63/63 [==============================] - 36s 570ms/step - loss: 0.3068 - val_loss: 4.5334\n",
      "Learning Rate for epoch: 68:  9.999998695775503e-08\n",
      "Epoch 69/100\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.3085\n",
      "Epoch 00069: ReduceLROnPlateau reducing learning rate to 9.999998695775504e-09.\n",
      "63/63 [==============================] - 36s 564ms/step - loss: 0.3082 - val_loss: 4.5020\n",
      "Learning Rate for epoch: 69:  9.99999905104687e-09\n",
      "Epoch 70/100\n",
      "63/63 [==============================] - 36s 566ms/step - loss: 0.3056 - val_loss: 4.5077\n",
      "Learning Rate for epoch: 70:  9.99999905104687e-09\n",
      "Epoch 71/100\n",
      "63/63 [==============================] - 36s 564ms/step - loss: 0.3078 - val_loss: 4.5237\n",
      "Learning Rate for epoch: 71:  9.99999905104687e-09\n",
      "Epoch 72/100\n",
      "63/63 [==============================] - 36s 565ms/step - loss: 0.3073 - val_loss: 4.5137\n",
      "Learning Rate for epoch: 72:  9.99999905104687e-09\n",
      "Epoch 73/100\n",
      "63/63 [==============================] - 36s 567ms/step - loss: 0.3069 - val_loss: 4.5262\n",
      "Learning Rate for epoch: 73:  9.99999905104687e-09\n",
      "Epoch 74/100\n",
      "63/63 [==============================] - 36s 565ms/step - loss: 0.3069 - val_loss: 4.5020\n",
      "Learning Rate for epoch: 74:  9.99999905104687e-09\n",
      "Epoch 75/100\n",
      "63/63 [==============================] - 36s 567ms/step - loss: 0.3057 - val_loss: 4.5104\n",
      "Learning Rate for epoch: 75:  9.99999905104687e-09\n",
      "Epoch 76/100\n",
      "63/63 [==============================] - 36s 564ms/step - loss: 0.3086 - val_loss: 4.4926\n",
      "Learning Rate for epoch: 76:  9.99999905104687e-09\n",
      "Epoch 77/100\n",
      "63/63 [==============================] - 36s 570ms/step - loss: 0.3072 - val_loss: 4.4750\n",
      "Learning Rate for epoch: 77:  9.99999905104687e-09\n",
      "Epoch 78/100\n",
      "63/63 [==============================] - 36s 565ms/step - loss: 0.3079 - val_loss: 4.4926\n",
      "Learning Rate for epoch: 78:  9.99999905104687e-09\n",
      "Epoch 79/100\n",
      "63/63 [==============================] - 36s 568ms/step - loss: 0.3073 - val_loss: 4.4993\n",
      "Learning Rate for epoch: 79:  9.99999905104687e-09\n",
      "Epoch 80/100\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.3067\n",
      "Epoch 00080: ReduceLROnPlateau reducing learning rate to 9.99999905104687e-10.\n",
      "63/63 [==============================] - 36s 566ms/step - loss: 0.3068 - val_loss: 4.4866\n",
      "Learning Rate for epoch: 80:  9.99999860695766e-10\n",
      "Epoch 81/100\n",
      "63/63 [==============================] - 36s 567ms/step - loss: 0.3077 - val_loss: 4.5132\n",
      "Learning Rate for epoch: 81:  9.99999860695766e-10\n",
      "Epoch 82/100\n",
      "63/63 [==============================] - 36s 569ms/step - loss: 0.3068 - val_loss: 4.4973\n",
      "Learning Rate for epoch: 82:  9.99999860695766e-10\n",
      "Epoch 83/100\n",
      "63/63 [==============================] - 36s 566ms/step - loss: 0.3046 - val_loss: 4.5067\n",
      "Learning Rate for epoch: 83:  9.99999860695766e-10\n",
      "Epoch 84/100\n",
      "63/63 [==============================] - 36s 567ms/step - loss: 0.3079 - val_loss: 4.4875\n",
      "Learning Rate for epoch: 84:  9.99999860695766e-10\n",
      "Epoch 85/100\n",
      "63/63 [==============================] - 36s 567ms/step - loss: 0.3083 - val_loss: 4.4837\n",
      "Learning Rate for epoch: 85:  9.99999860695766e-10\n",
      "Epoch 86/100\n",
      "63/63 [==============================] - 36s 569ms/step - loss: 0.3066 - val_loss: 4.5083\n",
      "Learning Rate for epoch: 86:  9.99999860695766e-10\n",
      "Epoch 87/100\n",
      "63/63 [==============================] - 36s 564ms/step - loss: 0.3092 - val_loss: 4.4838\n",
      "Learning Rate for epoch: 87:  9.99999860695766e-10\n",
      "Epoch 88/100\n",
      "63/63 [==============================] - 36s 564ms/step - loss: 0.3085 - val_loss: 4.5190\n",
      "Learning Rate for epoch: 88:  9.99999860695766e-10\n",
      "Epoch 89/100\n",
      "63/63 [==============================] - 35s 563ms/step - loss: 0.3058 - val_loss: 4.5255\n",
      "Learning Rate for epoch: 89:  9.99999860695766e-10\n",
      "Epoch 90/100\n",
      "63/63 [==============================] - 35s 563ms/step - loss: 0.3064 - val_loss: 4.5041\n",
      "Learning Rate for epoch: 90:  9.99999860695766e-10\n",
      "Epoch 91/100\n",
      "62/63 [============================>.] - ETA: 0s - loss: 0.3070\n",
      "Epoch 00091: ReduceLROnPlateau reducing learning rate to 9.999998606957661e-11.\n",
      "63/63 [==============================] - 35s 563ms/step - loss: 0.3072 - val_loss: 4.5158\n",
      "Learning Rate for epoch: 91:  9.999998745735539e-11\n",
      "Epoch 92/100\n",
      "63/63 [==============================] - 35s 563ms/step - loss: 0.3075 - val_loss: 4.5069\n",
      "Learning Rate for epoch: 92:  9.999998745735539e-11\n",
      "Epoch 93/100\n",
      "63/63 [==============================] - 36s 564ms/step - loss: 0.3066 - val_loss: 4.5057\n",
      "Learning Rate for epoch: 93:  9.999998745735539e-11\n",
      "Epoch 94/100\n",
      "63/63 [==============================] - 36s 568ms/step - loss: 0.3081 - val_loss: 4.4972\n",
      "Learning Rate for epoch: 94:  9.999998745735539e-11\n",
      "Epoch 95/100\n",
      "63/63 [==============================] - 36s 565ms/step - loss: 0.3073 - val_loss: 4.4676\n",
      "Learning Rate for epoch: 95:  9.999998745735539e-11\n",
      "Epoch 96/100\n",
      "63/63 [==============================] - 35s 562ms/step - loss: 0.3072 - val_loss: 4.5013\n",
      "Learning Rate for epoch: 96:  9.999998745735539e-11\n",
      "Epoch 97/100\n",
      "63/63 [==============================] - 36s 564ms/step - loss: 0.3076 - val_loss: 4.4812\n",
      "Learning Rate for epoch: 97:  9.999998745735539e-11\n",
      "Epoch 98/100\n",
      "63/63 [==============================] - 35s 563ms/step - loss: 0.3079 - val_loss: 4.4942\n",
      "Learning Rate for epoch: 98:  9.999998745735539e-11\n",
      "Epoch 99/100\n",
      "63/63 [==============================] - 36s 565ms/step - loss: 0.3056 - val_loss: 4.4671\n",
      "Learning Rate for epoch: 99:  9.999998745735539e-11\n",
      "Epoch 100/100\n",
      "63/63 [==============================] - 35s 563ms/step - loss: 0.3055 - val_loss: 4.5431\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6948a99bd0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Generators.\n",
    "train_gen = DataGenerator(eng_train, hindi_train, \n",
    "                          MAX_LEN_ENG, \n",
    "                          MAX_LEN_HIN,\n",
    "                          TARGET_VOCAB,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True)\n",
    "dev_gen = DataGenerator(eng_test, hindi_test, \n",
    "                        MAX_LEN_ENG, \n",
    "                        MAX_LEN_HIN,\n",
    "                        TARGET_VOCAB,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True)\n",
    "\n",
    "nmt_model.fit_generator(train_gen,\n",
    "                        steps_per_epoch=INPUT_VOCAB//batch_size,\n",
    "                        epochs=nb_epochs,\n",
    "                        validation_data=dev_gen,\n",
    "                        validation_steps=TARGET_VOCAB//batch_size,\n",
    "                        verbose=1,\n",
    "                        shuffle=True,\n",
    "                        callbacks=[MC, _learning_rate(), R_LR, LOG])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an inference encoder model from the tensors we previously declared\n",
    "inf_encoder_model = Model(encoder_inputs, [encoder_outputs, state_h, state_c])\n",
    "\n",
    "# Generate a new set of tensors for our new inference decoder. Note that we are using new tensors, \n",
    "inf_decoder_inputs = Input(shape=(None,), name=\"inf_decoder_inputs\")\n",
    "# We'll need to force feed the two state variables into the decoder each step.\n",
    "state_input_h = Input(shape=(latent_dim*2,), name=\"state_input_h\")\n",
    "state_input_c = Input(shape=(latent_dim*2,), name=\"state_input_c\")\n",
    "decoder_res, decoder_h, decoder_c = decoder_lstm_layer2(decoder_lstm_layer1(\n",
    "                                            decoder_emb_layer(inf_decoder_inputs), \n",
    "                                            initial_state=[state_input_h, state_input_c]))\n",
    "inf_decoder_output = decoder_dense2(decoder_dense1(decoder_res))\n",
    "inf_decoder_model = Model([inf_decoder_inputs, state_input_h, state_input_c],\n",
    "                         [inf_decoder_output, decoder_h, decoder_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "eg_gen = DataGenerator(eng_train, hindi_train, \n",
    "                          MAX_LEN_ENG, \n",
    "                          MAX_LEN_HIN,\n",
    "                          TARGET_VOCAB,\n",
    "                          batch_size=1,\n",
    "                          shuffle=True)\n",
    "\n",
    "input_seq, target_output = eg_gen.__getitem__(index=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the input as state vectors (h, c)\n",
    "d_emb, d_h, d_c = inf_encoder_model.predict(input_seq)\n",
    "\n",
    "# Populate the first character of target sequence with the start character.\n",
    "target_seq = np.zeros((1, 1))\n",
    "target_seq[0, 0] = hin_vocab_idx['<BOS>']\n",
    "\n",
    "decode = False\n",
    "decoded_sentence = ''\n",
    "while not decode:\n",
    "    output_tokens, h, c = inf_decoder_model.predict([target_seq, d_h, d_c])\n",
    "    \n",
    "    # pick word with max probability\n",
    "    token_idx = np.argmax(output_tokens.flatten())\n",
    "    char = idx_vocab_hin[token_idx]\n",
    "    decoded_sentence += ' ' + char\n",
    "    \n",
    "    if char == '<EOS>' or len(decoded_sentence) > 50:\n",
    "        decode = True\n",
    "        \n",
    "    target_seq[0, 0] = hin_vocab_idx[char]\n",
    "    \n",
    "    encode_state_vectors = [h, c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ३०० ३०० ३०० ३०० ३०० ३०० ३०० ३०० ३०० ३०० ३०० ३०० ३००'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
